{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli download lmsys/vicuna-7b-v1.3 --local-dir '/root/SD-research/base_models/vicuna'\n",
    "# !huggingface-cli download yuhuili/EAGLE-Vicuna-7B-v1.3 --local-dir '/root/SD-research/ea_models/vicuna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|██████████████████████| 16/16 [00:00<00:00, 5904.87it/s]\n",
      "/root/SD-research/base_models/llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|                                   | 0/4 [00:00<?, ?it/s]Downloading 'README.md' to '/root/SD-research/ea_models/llama2/.cache/huggingface/download/README.md.154df8298fab5ecf322016157858e08cd1bccbe1.incomplete'\n",
      "Downloading '.gitattributes' to '/root/SD-research/ea_models/llama2/.cache/huggingface/download/.gitattributes.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "\n",
      "README.md: 100%|██████████████████████████████| 28.0/28.0 [00:00<00:00, 263kB/s]\u001b[A\n",
      "Download complete. Moving file to /root/SD-research/ea_models/llama2/README.md\n",
      "\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 22.7MB/s]\u001b[A\n",
      "Download complete. Moving file to /root/SD-research/ea_models/llama2/.gitattributes\n",
      "Fetching 4 files:  25%|██████▊                    | 1/4 [00:00<00:01,  1.97it/s]Downloading 'pytorch_model.bin' to '/root/SD-research/ea_models/llama2/.cache/huggingface/download/pytorch_model.bin.3f1e4f6bef7befeba9d08754f98088799d754250b20fc61a46081e8ea0ca978b.incomplete'\n",
      "Downloading 'config.json' to '/root/SD-research/ea_models/llama2/.cache/huggingface/download/config.json.80912d00ef3abadf69eef0864849d0c7beea2cdc.incomplete'\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 560/560 [00:00<00:00, 4.48MB/s]\u001b[A\n",
      "Download complete. Moving file to /root/SD-research/ea_models/llama2/config.json\n",
      "Fetching 4 files:  75%|████████████████████▎      | 3/4 [00:10<00:03,  3.83s/it]\n",
      "pytorch_model.bin:   0%|                            | 0.00/1.47G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model.bin:   1%|▏                  | 10.5M/1.47G [00:00<01:44, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|▎                  | 21.0M/1.47G [00:01<01:05, 22.1MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▍                  | 31.5M/1.47G [00:01<01:19, 18.0MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|▌                  | 41.9M/1.47G [00:02<01:27, 16.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▋                  | 52.4M/1.47G [00:03<01:31, 15.4MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▊                  | 62.9M/1.47G [00:03<01:33, 15.0MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|▉                  | 73.4M/1.47G [00:04<01:34, 14.7MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|█                  | 83.9M/1.47G [00:05<01:35, 14.5MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|█▏                 | 94.4M/1.47G [00:06<01:35, 14.4MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|█▍                  | 105M/1.47G [00:06<01:35, 14.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|█▌                  | 115M/1.47G [00:07<01:35, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|█▋                  | 126M/1.47G [00:08<01:34, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|█▊                  | 136M/1.47G [00:09<01:34, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|█▉                  | 147M/1.47G [00:09<01:33, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|██▏                 | 157M/1.47G [00:10<01:40, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|██▎                 | 168M/1.47G [00:11<01:37, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|██▍                 | 178M/1.47G [00:12<01:35, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|██▌                 | 189M/1.47G [00:13<01:33, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|██▋                 | 199M/1.47G [00:13<01:31, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|██▊                 | 210M/1.47G [00:14<01:31, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|██▉                 | 220M/1.47G [00:15<01:29, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|███▏                | 231M/1.47G [00:16<01:28, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|███▎                | 241M/1.47G [00:16<01:27, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|███▍                | 252M/1.47G [00:17<01:26, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|███▌                | 262M/1.47G [00:18<01:25, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|███▋                | 273M/1.47G [00:19<01:24, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|███▊                | 283M/1.47G [00:19<01:23, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|███▉                | 294M/1.47G [00:20<01:29, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|████▏               | 304M/1.47G [00:21<01:27, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|████▎               | 315M/1.47G [00:22<01:24, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|████▍               | 325M/1.47G [00:22<01:23, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|████▌               | 336M/1.47G [00:23<01:21, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|████▋               | 346M/1.47G [00:24<01:20, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|████▊               | 357M/1.47G [00:25<01:19, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|████▉               | 367M/1.47G [00:25<01:18, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|█████▏              | 377M/1.47G [00:26<01:17, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|█████▎              | 388M/1.47G [00:27<01:16, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|█████▍              | 398M/1.47G [00:28<01:15, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|█████▌              | 409M/1.47G [00:28<01:15, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|█████▋              | 419M/1.47G [00:29<01:14, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|█████▊              | 430M/1.47G [00:30<01:13, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|█████▉              | 440M/1.47G [00:31<01:18, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|██████▏             | 451M/1.47G [00:32<01:16, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|██████▎             | 461M/1.47G [00:32<01:14, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|██████▍             | 472M/1.47G [00:33<01:12, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|██████▌             | 482M/1.47G [00:34<01:11, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|██████▋             | 493M/1.47G [00:35<01:10, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|██████▊             | 503M/1.47G [00:35<01:09, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|██████▉             | 514M/1.47G [00:36<01:08, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|███████▏            | 524M/1.47G [00:37<01:07, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|███████▎            | 535M/1.47G [00:37<01:06, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███████▍            | 545M/1.47G [00:38<01:05, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███████▌            | 556M/1.47G [00:39<01:04, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███████▋            | 566M/1.47G [00:40<01:03, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███████▊            | 577M/1.47G [00:40<01:03, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|███████▉            | 587M/1.47G [00:41<01:02, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|████████▏           | 598M/1.47G [00:42<01:01, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|████████▎           | 608M/1.47G [00:43<01:01, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|████████▍           | 619M/1.47G [00:44<01:05, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████████▌           | 629M/1.47G [00:44<01:02, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████████▋           | 640M/1.47G [00:45<01:00, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████████▊           | 650M/1.47G [00:46<00:59, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████████▉           | 661M/1.47G [00:47<00:58, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|█████████▏          | 671M/1.47G [00:47<00:57, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|█████████▎          | 682M/1.47G [00:48<00:56, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|█████████▍          | 692M/1.47G [00:49<00:55, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|█████████▌          | 703M/1.47G [00:50<00:54, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|█████████▋          | 713M/1.47G [00:50<00:53, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|█████████▊          | 724M/1.47G [00:51<00:52, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|█████████▉          | 734M/1.47G [00:52<00:52, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|██████████▏         | 744M/1.47G [00:53<00:51, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|██████████▎         | 755M/1.47G [00:53<00:50, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|██████████▍         | 765M/1.47G [00:54<00:53, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|██████████▌         | 776M/1.47G [00:55<00:51, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|██████████▋         | 786M/1.47G [00:56<00:50, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|██████████▊         | 797M/1.47G [00:56<00:48, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|██████████▉         | 807M/1.47G [00:57<00:47, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|███████████▏        | 818M/1.47G [00:58<00:46, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|███████████▎        | 828M/1.47G [00:59<00:45, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|███████████▍        | 839M/1.47G [00:59<00:44, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|███████████▌        | 849M/1.47G [01:00<00:44, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|███████████▋        | 860M/1.47G [01:01<00:43, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|███████████▊        | 870M/1.47G [01:02<00:42, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|███████████▉        | 881M/1.47G [01:02<00:41, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|████████████▏       | 891M/1.47G [01:03<00:40, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|████████████▎       | 902M/1.47G [01:04<00:43, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|████████████▍       | 912M/1.47G [01:05<00:41, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|████████████▌       | 923M/1.47G [01:06<00:40, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|████████████▋       | 933M/1.47G [01:06<00:39, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|████████████▊       | 944M/1.47G [01:07<00:37, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|████████████▉       | 954M/1.47G [01:08<00:36, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|█████████████▏      | 965M/1.47G [01:09<00:35, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|█████████████▎      | 975M/1.47G [01:09<00:35, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|█████████████▍      | 986M/1.47G [01:10<00:34, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|█████████████▌      | 996M/1.47G [01:11<00:33, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|█████████████      | 1.01G/1.47G [01:11<00:32, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|█████████████▏     | 1.02G/1.47G [01:12<00:32, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|█████████████▎     | 1.03G/1.47G [01:13<00:31, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▍     | 1.04G/1.47G [01:14<00:30, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▌     | 1.05G/1.47G [01:15<00:32, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|█████████████▋     | 1.06G/1.47G [01:15<00:30, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|█████████████▊     | 1.07G/1.47G [01:16<00:29, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|█████████████▉     | 1.08G/1.47G [01:17<00:28, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|██████████████     | 1.09G/1.47G [01:18<00:27, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|██████████████▏    | 1.10G/1.47G [01:18<00:26, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▍    | 1.11G/1.47G [01:19<00:25, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▌    | 1.12G/1.47G [01:20<00:24, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|██████████████▋    | 1.13G/1.47G [01:21<00:23, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|██████████████▊    | 1.14G/1.47G [01:21<00:23, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|██████████████▉    | 1.15G/1.47G [01:22<00:22, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|███████████████    | 1.16G/1.47G [01:23<00:21, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|███████████████▏   | 1.17G/1.47G [01:24<00:20, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▎   | 1.18G/1.47G [01:25<00:21, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▍   | 1.20G/1.47G [01:25<00:20, 13.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|███████████████▌   | 1.21G/1.47G [01:26<00:19, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|███████████████▋   | 1.22G/1.47G [01:27<00:18, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|███████████████▉   | 1.23G/1.47G [01:28<00:17, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|████████████████   | 1.24G/1.47G [01:28<00:16, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████████████▏  | 1.25G/1.47G [01:29<00:15, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████████████▎  | 1.26G/1.47G [01:30<00:15, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████████████▍  | 1.27G/1.47G [01:30<00:14, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████████████▌  | 1.28G/1.47G [01:31<00:13, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████████████▋  | 1.29G/1.47G [01:32<00:12, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████████████▊  | 1.30G/1.47G [01:33<00:11, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████████████▉  | 1.31G/1.47G [01:33<00:11, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|█████████████████  | 1.32G/1.47G [01:34<00:10, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████████████▏ | 1.33G/1.47G [01:35<00:09, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████████████▎ | 1.34G/1.47G [01:36<00:08, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████████████▌ | 1.35G/1.47G [01:36<00:08, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████████████▋ | 1.36G/1.47G [01:37<00:07, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████████████▊ | 1.37G/1.47G [01:38<00:06, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████████████▉ | 1.38G/1.47G [01:39<00:05, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|██████████████████ | 1.39G/1.47G [01:39<00:05, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|██████████████████▏| 1.41G/1.47G [01:40<00:04, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|██████████████████▎| 1.42G/1.47G [01:41<00:03, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|██████████████████▍| 1.43G/1.47G [01:42<00:02, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|██████████████████▌| 1.44G/1.47G [01:42<00:02, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|██████████████████▋| 1.45G/1.47G [01:43<00:01, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|██████████████████▊| 1.46G/1.47G [01:44<00:00, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|███████████████████| 1.47G/1.47G [01:45<00:00, 14.0MB/s]\u001b[A\n",
      "Download complete. Moving file to /root/SD-research/ea_models/llama2/pytorch_model.bin\n",
      "Fetching 4 files: 100%|███████████████████████████| 4/4 [01:55<00:00, 28.91s/it]\n",
      "/root/SD-research/ea_models/llama2\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir '/root/SD-research/base_models/llama2'\n",
    "!huggingface-cli download yuhuili/EAGLE-llama2-chat-7B --local-dir '/root/SD-research/ea_models/llama2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import yaml\n",
    "import os\n",
    "from eagle.model.ea_model import EaModel\n",
    "from fastchat.model import get_conversation_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EaModel.from_pretrained(\n",
    "    base_model_path='/root/SD-research/base_models/vicuna',\n",
    "    ea_model_path='/root/SD-research/ea_models/vicuna',\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EaModel.naive_generate() got multiple values for argument 'temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 24\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# output_ids=model.eagenerate(input_s.input_ids,input_s.attention_mask,temperature=0.0,max_new_tokens=512,top_k=15)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# output=model.tokenizer.batch_decode(output_ids)    \u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# inference_time = (datetime.now() - start_time).total_seconds()\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(inference_time)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m start_time_v \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()    \n\u001b[0;32m---> 24\u001b[0m output_ids, new_token, idx\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnaive_generate(input_s\u001b[38;5;241m.\u001b[39minput_ids,input_s\u001b[38;5;241m.\u001b[39mattention_mask,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m inference_time_v \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time_v)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(inference_time_v)\n",
      "File \u001b[0;32m~/miniconda3/envs/sd/lib/python3.13/site-packages/torch/utils/_contextlib.py:27\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerator_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 27\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Generators are suspended and unsuspended at `yield`, hence we\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# make sure the grad mode is properly set every time the execution\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# flow returns into the wrapped generator and restored when it\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# returns through our `yield` to our caller (see PR #49017).\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: EaModel.naive_generate() got multiple values for argument 'temperature'"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(4)):\n",
    "    prompts = []\n",
    "    # load input data\n",
    "    segment_string = f'{500*i}~{500*(i+1)}'\n",
    "    input_filename = f\"../data/ArXiv/4K/ArXiv_4K_{segment_string}.json\"\n",
    "    with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "        raw_input = json.load(f)\n",
    "        input_data = [item['final_input'] for item in raw_input]\n",
    "        \n",
    "    for item in input_data :\n",
    "        conv = get_conversation_template(\"vicuna\")\n",
    "        conv.append_message(conv.roles[0], item)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompts.append(conv.get_prompt())\n",
    "        \n",
    "    start_time = datetime.now()\n",
    "    input_s=model.tokenizer(prompts,return_tensors=\"pt\",padding=True).to(\"cuda\")\n",
    "    output_ids=model.eagenerate(input_s.input_ids,input_s.attention_mask,temperature=0.0,max_new_tokens=512,top_k=15)\n",
    "    output=model.tokenizer.batch_decode(output_ids)    \n",
    "    inference_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(inference_time)\n",
    "    \n",
    "    start_time_v = datetime.now()    \n",
    "    output_ids, new_token, idx=model.naive_generate(input_s.input_ids,input_s.attention_mask,temperature=0.0,max_new_tokens=512,top_k=15,log=True)\n",
    "    inference_time_v = (datetime.now() - start_time_v).total_seconds()\n",
    "    print(inference_time_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will receive the full text of a scientific paper. Your task is to write a formal abstract (under 300 words) that includes:\n",
      "\n",
      "- The primary objective or research question.\n",
      "- A brief description of the methodology.\n",
      "- The main findings or results.\n",
      "- The significance or implications of the findings.\n",
      "\n",
      "Use clear, professional, and concise language suitable for publication.\n",
      "\n",
      "### Paper content\n",
      "\n",
      "there are several reasons to believe that a population of intergalactic globular clusters ( igcs ) should exist outside of galaxies :    \\(1 ) the jeans mass at recombination was @xmath0 solar masses , and hence globular cluster sized objects could have formed wherever the local density of matter was high enough . \\(2 ) many galaxies may have met their demise over a hubble time as a result of collisions and tidal disruption . globular clusters are likely to survive the disruption of their parent galaxy , resulting in the gradual accumulation of a population of igcs . intergalactic stars , planetary nebulae , supernovae and hii regions have already been found ; it would be surprising if there were no igcs . \\(3 ) the existence of igcs might explain high specific frequencies , bimodal globular cluster metallicity distributions and other current puzzles in the study of globular cluster systems . jordn et al . ( 2003 ) reported a tentative detection of igcs in the center of the rich galaxy cluster a1185 ( @xmath1 ) based on @xmath2-band images obtained with wfpc2 on the hubble space telescope . we ( ct , jordn , marzke , west ) recently obtained very deep , multicolored ( @xmath3 and @xmath2 ) images of the same a1185 field using hst with the new acs . the goals of these new observations are to 1 ) detect the peak of the assumed universal gaussian - like globular cluster luminosity function ( which should occur at @xmath4 at a1185 s distance ) and thereby confirm that these candidate igcs are bona fide globular clusters and 2 ) use color information to infer their metallicities . preliminary analysis indicates that we are reaching sufficiently faint magnitudes to reliably detect the luminosity function turnover . the number and colors ( metallicities ) of igcs will provide constraints on the number and types of galaxies that have been destroyed or stripped over a hubble time .    using the keck telescope , we ( ferguson , gregg , tanvir , von hippel , west ) recently measured the redshift of a candidate igc in the nearby virgo galaxy cluster that was found serendipitously on an hst image obtained for another project . preliminary data reductions show that this object , which is slightly resolved in the hst image and appears to be a distant globular cluster , has a recessional velocity of @xmath5 km / s , and hence is most likely in the virgo cluster . its apparent magnitude , @xmath6 , is consistent with it being a bright globular cluster . using telescopes on mauna kea we have since obtained optical and nir colors of this object , as well as a medium - resolution spectrum that should yield its velocity dispersion . these data are presently being analyzed .\n",
      "\n",
      "### Output\n",
      "\n",
      "Abstract: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: You will receive the full text of a scientific paper. Your task is to write a formal abstract (under 300 words) that includes:\\n\\n- The primary objective or research question.\\n- A brief description of the methodology.\\n- The main findings or results.\\n- The significance or implications of the findings.\\n\\nUse clear, professional, and concise language suitable for publication.\\n\\n### Paper content\\n\\nthere are several reasons to believe that a population of intergalactic globular clusters ( igcs ) should exist outside of galaxies :    \\\\(1 ) the jeans mass at recombination was @xmath0 solar masses , and hence globular cluster sized objects could have formed wherever the local density of matter was high enough . \\\\(2 ) many galaxies may have met their demise over a hubble time as a result of collisions and tidal disruption . globular clusters are likely to survive the disruption of their parent galaxy , resulting in the gradual accumulation of a population of igcs . intergalactic stars , planetary nebulae , supernovae and hii regions have already been found ; it would be surprising if there were no igcs . \\\\(3 ) the existence of igcs might explain high specific frequencies , bimodal globular cluster metallicity distributions and other current puzzles in the study of globular cluster systems . jordn et al . ( 2003 ) reported a tentative detection of igcs in the center of the rich galaxy cluster a1185 ( @xmath1 ) based on @xmath2-band images obtained with wfpc2 on the hubble space telescope . we ( ct , jordn , marzke , west ) recently obtained very deep , multicolored ( @xmath3 and @xmath2 ) images of the same a1185 field using hst with the new acs . the goals of these new observations are to 1 ) detect the peak of the assumed universal gaussian - like globular cluster luminosity function ( which should occur at @xmath4 at a1185 s distance ) and thereby confirm that these candidate igcs are bona fide globular clusters and 2 ) use color information to infer their metallicities . preliminary analysis indicates that we are reaching sufficiently faint magnitudes to reliably detect the luminosity function turnover . the number and colors ( metallicities ) of igcs will provide constraints on the number and types of galaxies that have been destroyed or stripped over a hubble time .    using the keck telescope , we ( ferguson , gregg , tanvir , von hippel , west ) recently measured the redshift of a candidate igc in the nearby virgo galaxy cluster that was found serendipitously on an hst image obtained for another project . preliminary data reductions show that this object , which is slightly resolved in the hst image and appears to be a distant globular cluster , has a recessional velocity of @xmath5 km / s , and hence is most likely in the virgo cluster . its apparent magnitude , @xmath6 , is consistent with it being a bright globular cluster . using telescopes on mauna kea we have since obtained optical and nir colors of this object , as well as a medium - resolution spectrum that should yield its velocity dispersion . these data are presently being analyzed .\\n\\n### Output\\n\\nAbstract:  ASSISTANT:\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "segment_string = f'{500*i}~{500*(i+1)}'\n",
    "input_filename = f\"../data/ArXiv/4K/ArXiv_4K_{segment_string}.json\"\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    raw_input = json.load(f)\n",
    "    input_data = [item['final_input'] for item in raw_input]        \n",
    "    print(input_data[0])\n",
    "\n",
    "conv = get_conversation_template(\"vicuna\")\n",
    "conv.append_message(conv.roles[0], input_data[0])\n",
    "conv.append_message(conv.roles[1],None)\n",
    "prompt = conv.get_prompt()\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_ids=model.tokenizer([prompt]).input_ids\n",
    "input_ids = torch.as_tensor(input_ids).cuda()\n",
    "output_ids=model.eagenerate(input_ids,temperature=0.5,max_new_tokens=512)\n",
    "output=model.tokenizer.decode(output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   319, 13563,  ...,  6757, 29889,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: You will receive the full text of a scientific paper. Your task is to write a formal abstract (under 300 words) that includes:\\n\\n- The primary objective or research question.\\n- A brief description of the methodology.\\n- The main findings or results.\\n- The significance or implications of the findings.\\n\\nUse clear, professional, and concise language suitable for publication.\\n\\n### Paper content\\n\\nthere are several reasons to believe that a population of intergalactic globular clusters ( igcs ) should exist outside of galaxies :    \\\\(1 ) the jeans mass at recombination was @xmath0 solar masses , and hence globular cluster sized objects could have formed wherever the local density of matter was high enough . \\\\(2 ) many galaxies may have met their demise over a hubble time as a result of collisions and tidal disruption . globular clusters are likely to survive the disruption of their parent galaxy , resulting in the gradual accumulation of a population of igcs . intergalactic stars , planetary nebulae , supernovae and hii regions have already been found ; it would be surprising if there were no igcs . \\\\(3 ) the existence of igcs might explain high specific frequencies , bimodal globular cluster metallicity distributions and other current puzzles in the study of globular cluster systems . jordn et al . ( 2003 ) reported a tentative detection of igcs in the center of the rich galaxy cluster a1185 ( @xmath1 ) based on @xmath2-band images obtained with wfpc2 on the hubble space telescope . we ( ct , jordn , marzke , west ) recently obtained very deep , multicolored ( @xmath3 and @xmath2 ) images of the same a1185 field using hst with the new acs . the goals of these new observations are to 1 ) detect the peak of the assumed universal gaussian - like globular cluster luminosity function ( which should occur at @xmath4 at a1185 s distance ) and thereby confirm that these candidate igcs are bona fide globular clusters and 2 ) use color information to infer their metallicities . preliminary analysis indicates that we are reaching sufficiently faint magnitudes to reliably detect the luminosity function turnover . the number and colors ( metallicities ) of igcs will provide constraints on the number and types of galaxies that have been destroyed or stripped over a hubble time .    using the keck telescope , we ( ferguson , gregg , tanvir , von hippel , west ) recently measured the redshift of a candidate igc in the nearby virgo galaxy cluster that was found serendipitously on an hst image obtained for another project . preliminary data reductions show that this object , which is slightly resolved in the hst image and appears to be a distant globular cluster , has a recessional velocity of @xmath5 km / s , and hence is most likely in the virgo cluster . its apparent magnitude , @xmath6 , is consistent with it being a bright globular cluster . using telescopes on mauna kea we have since obtained optical and nir colors of this object , as well as a medium - resolution spectrum that should yield its velocity dispersion . these data are presently being analyzed .\\n\\n### Output\\n\\nAbstract:  ASSISTANT:\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The primary objective of this study was to investigate the existence of intergalactic globular clusters (IGCs) outside of galaxies. The research question was whether IGCs exist and could be detected in the field of the rich galaxy cluster A1185. The methodology involved acquiring deep, multicolored images of the A1185 field using the Hubble Space Telescope (HST) and the new Advanced Camera for Surveys (ACS). The authors aimed to detect the peak of the assumed universal Gaussian-like globular cluster luminosity function and use color information to infer the metallicities of the IGCs. The main findings were that the authors were able to reach sufficiently faint magnitudes to reliably detect the luminosity function turnover. Preliminary analysis of the data revealed that the number and colors of the IGCs will provide constraints on the number and types of galaxies that have been destroyed or stripped over a Hubble time. The significance of the findings is that the existence of IGCs might explain high specific frequencies, bimodal globular cluster metallicity distributions, and other current puzzles in the study of globular cluster systems.</s>'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('ASSISTANT: ',output)[-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
