{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli download lmsys/vicuna-7b-v1.3 --local-dir '/root/SD-research/base_models/vicuna'\n",
    "# !huggingface-cli download yuhuili/EAGLE-Vicuna-7B-v1.3 --local-dir '/root/SD-research/ea_models/vicuna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|██████████████████████| 16/16 [00:00<00:00, 5904.87it/s]\n",
      "/root/SD-research/base_models/llama2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|                                   | 0/4 [00:00<?, ?it/s]Downloading 'README.md' to '/root/SD-research/ea_models/llama2/.cache/huggingface/download/README.md.154df8298fab5ecf322016157858e08cd1bccbe1.incomplete'\n",
      "Downloading '.gitattributes' to '/root/SD-research/ea_models/llama2/.cache/huggingface/download/.gitattributes.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
      "\n",
      "README.md: 100%|██████████████████████████████| 28.0/28.0 [00:00<00:00, 263kB/s]\u001b[A\n",
      "Download complete. Moving file to /root/SD-research/ea_models/llama2/README.md\n",
      "\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 22.7MB/s]\u001b[A\n",
      "Download complete. Moving file to /root/SD-research/ea_models/llama2/.gitattributes\n",
      "Fetching 4 files:  25%|██████▊                    | 1/4 [00:00<00:01,  1.97it/s]Downloading 'pytorch_model.bin' to '/root/SD-research/ea_models/llama2/.cache/huggingface/download/pytorch_model.bin.3f1e4f6bef7befeba9d08754f98088799d754250b20fc61a46081e8ea0ca978b.incomplete'\n",
      "Downloading 'config.json' to '/root/SD-research/ea_models/llama2/.cache/huggingface/download/config.json.80912d00ef3abadf69eef0864849d0c7beea2cdc.incomplete'\n",
      "\n",
      "config.json: 100%|█████████████████████████████| 560/560 [00:00<00:00, 4.48MB/s]\u001b[A\n",
      "Download complete. Moving file to /root/SD-research/ea_models/llama2/config.json\n",
      "Fetching 4 files:  75%|████████████████████▎      | 3/4 [00:10<00:03,  3.83s/it]\n",
      "pytorch_model.bin:   0%|                            | 0.00/1.47G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model.bin:   1%|▏                  | 10.5M/1.47G [00:00<01:44, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:   1%|▎                  | 21.0M/1.47G [00:01<01:05, 22.1MB/s]\u001b[A\n",
      "pytorch_model.bin:   2%|▍                  | 31.5M/1.47G [00:01<01:19, 18.0MB/s]\u001b[A\n",
      "pytorch_model.bin:   3%|▌                  | 41.9M/1.47G [00:02<01:27, 16.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▋                  | 52.4M/1.47G [00:03<01:31, 15.4MB/s]\u001b[A\n",
      "pytorch_model.bin:   4%|▊                  | 62.9M/1.47G [00:03<01:33, 15.0MB/s]\u001b[A\n",
      "pytorch_model.bin:   5%|▉                  | 73.4M/1.47G [00:04<01:34, 14.7MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|█                  | 83.9M/1.47G [00:05<01:35, 14.5MB/s]\u001b[A\n",
      "pytorch_model.bin:   6%|█▏                 | 94.4M/1.47G [00:06<01:35, 14.4MB/s]\u001b[A\n",
      "pytorch_model.bin:   7%|█▍                  | 105M/1.47G [00:06<01:35, 14.3MB/s]\u001b[A\n",
      "pytorch_model.bin:   8%|█▌                  | 115M/1.47G [00:07<01:35, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|█▋                  | 126M/1.47G [00:08<01:34, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:   9%|█▊                  | 136M/1.47G [00:09<01:34, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  10%|█▉                  | 147M/1.47G [00:09<01:33, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|██▏                 | 157M/1.47G [00:10<01:40, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  11%|██▎                 | 168M/1.47G [00:11<01:37, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  12%|██▍                 | 178M/1.47G [00:12<01:35, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  13%|██▌                 | 189M/1.47G [00:13<01:33, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|██▋                 | 199M/1.47G [00:13<01:31, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  14%|██▊                 | 210M/1.47G [00:14<01:31, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  15%|██▉                 | 220M/1.47G [00:15<01:29, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|███▏                | 231M/1.47G [00:16<01:28, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  16%|███▎                | 241M/1.47G [00:16<01:27, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  17%|███▍                | 252M/1.47G [00:17<01:26, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  18%|███▌                | 262M/1.47G [00:18<01:25, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|███▋                | 273M/1.47G [00:19<01:24, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  19%|███▊                | 283M/1.47G [00:19<01:23, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  20%|███▉                | 294M/1.47G [00:20<01:29, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|████▏               | 304M/1.47G [00:21<01:27, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  21%|████▎               | 315M/1.47G [00:22<01:24, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  22%|████▍               | 325M/1.47G [00:22<01:23, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  23%|████▌               | 336M/1.47G [00:23<01:21, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|████▋               | 346M/1.47G [00:24<01:20, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  24%|████▊               | 357M/1.47G [00:25<01:19, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  25%|████▉               | 367M/1.47G [00:25<01:18, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|█████▏              | 377M/1.47G [00:26<01:17, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  26%|█████▎              | 388M/1.47G [00:27<01:16, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  27%|█████▍              | 398M/1.47G [00:28<01:15, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  28%|█████▌              | 409M/1.47G [00:28<01:15, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|█████▋              | 419M/1.47G [00:29<01:14, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  29%|█████▊              | 430M/1.47G [00:30<01:13, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  30%|█████▉              | 440M/1.47G [00:31<01:18, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|██████▏             | 451M/1.47G [00:32<01:16, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  31%|██████▎             | 461M/1.47G [00:32<01:14, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  32%|██████▍             | 472M/1.47G [00:33<01:12, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  33%|██████▌             | 482M/1.47G [00:34<01:11, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|██████▋             | 493M/1.47G [00:35<01:10, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  34%|██████▊             | 503M/1.47G [00:35<01:09, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  35%|██████▉             | 514M/1.47G [00:36<01:08, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|███████▏            | 524M/1.47G [00:37<01:07, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  36%|███████▎            | 535M/1.47G [00:37<01:06, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  37%|███████▍            | 545M/1.47G [00:38<01:05, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  38%|███████▌            | 556M/1.47G [00:39<01:04, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███████▋            | 566M/1.47G [00:40<01:03, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  39%|███████▊            | 577M/1.47G [00:40<01:03, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  40%|███████▉            | 587M/1.47G [00:41<01:02, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|████████▏           | 598M/1.47G [00:42<01:01, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  41%|████████▎           | 608M/1.47G [00:43<01:01, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  42%|████████▍           | 619M/1.47G [00:44<01:05, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  43%|████████▌           | 629M/1.47G [00:44<01:02, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████████▋           | 640M/1.47G [00:45<01:00, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  44%|████████▊           | 650M/1.47G [00:46<00:59, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  45%|████████▉           | 661M/1.47G [00:47<00:58, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|█████████▏          | 671M/1.47G [00:47<00:57, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  46%|█████████▎          | 682M/1.47G [00:48<00:56, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  47%|█████████▍          | 692M/1.47G [00:49<00:55, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  48%|█████████▌          | 703M/1.47G [00:50<00:54, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|█████████▋          | 713M/1.47G [00:50<00:53, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  49%|█████████▊          | 724M/1.47G [00:51<00:52, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  50%|█████████▉          | 734M/1.47G [00:52<00:52, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|██████████▏         | 744M/1.47G [00:53<00:51, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  51%|██████████▎         | 755M/1.47G [00:53<00:50, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  52%|██████████▍         | 765M/1.47G [00:54<00:53, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  53%|██████████▌         | 776M/1.47G [00:55<00:51, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|██████████▋         | 786M/1.47G [00:56<00:50, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  54%|██████████▊         | 797M/1.47G [00:56<00:48, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  55%|██████████▉         | 807M/1.47G [00:57<00:47, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|███████████▏        | 818M/1.47G [00:58<00:46, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  56%|███████████▎        | 828M/1.47G [00:59<00:45, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  57%|███████████▍        | 839M/1.47G [00:59<00:44, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  58%|███████████▌        | 849M/1.47G [01:00<00:44, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|███████████▋        | 860M/1.47G [01:01<00:43, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  59%|███████████▊        | 870M/1.47G [01:02<00:42, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  60%|███████████▉        | 881M/1.47G [01:02<00:41, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|████████████▏       | 891M/1.47G [01:03<00:40, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  61%|████████████▎       | 902M/1.47G [01:04<00:43, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  62%|████████████▍       | 912M/1.47G [01:05<00:41, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  63%|████████████▌       | 923M/1.47G [01:06<00:40, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|████████████▋       | 933M/1.47G [01:06<00:39, 13.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  64%|████████████▊       | 944M/1.47G [01:07<00:37, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  65%|████████████▉       | 954M/1.47G [01:08<00:36, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|█████████████▏      | 965M/1.47G [01:09<00:35, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  66%|█████████████▎      | 975M/1.47G [01:09<00:35, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  67%|█████████████▍      | 986M/1.47G [01:10<00:34, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  68%|█████████████▌      | 996M/1.47G [01:11<00:33, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|█████████████      | 1.01G/1.47G [01:11<00:32, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  69%|█████████████▏     | 1.02G/1.47G [01:12<00:32, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  70%|█████████████▎     | 1.03G/1.47G [01:13<00:31, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▍     | 1.04G/1.47G [01:14<00:30, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  71%|█████████████▌     | 1.05G/1.47G [01:15<00:32, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  72%|█████████████▋     | 1.06G/1.47G [01:15<00:30, 13.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  73%|█████████████▊     | 1.07G/1.47G [01:16<00:29, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|█████████████▉     | 1.08G/1.47G [01:17<00:28, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  74%|██████████████     | 1.09G/1.47G [01:18<00:27, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  75%|██████████████▏    | 1.10G/1.47G [01:18<00:26, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▍    | 1.11G/1.47G [01:19<00:25, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  76%|██████████████▌    | 1.12G/1.47G [01:20<00:24, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  77%|██████████████▋    | 1.13G/1.47G [01:21<00:23, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  78%|██████████████▊    | 1.14G/1.47G [01:21<00:23, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|██████████████▉    | 1.15G/1.47G [01:22<00:22, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  79%|███████████████    | 1.16G/1.47G [01:23<00:21, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  80%|███████████████▏   | 1.17G/1.47G [01:24<00:20, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▎   | 1.18G/1.47G [01:25<00:21, 13.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  81%|███████████████▍   | 1.20G/1.47G [01:25<00:20, 13.3MB/s]\u001b[A\n",
      "pytorch_model.bin:  82%|███████████████▌   | 1.21G/1.47G [01:26<00:19, 13.6MB/s]\u001b[A\n",
      "pytorch_model.bin:  83%|███████████████▋   | 1.22G/1.47G [01:27<00:18, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|███████████████▉   | 1.23G/1.47G [01:28<00:17, 13.8MB/s]\u001b[A\n",
      "pytorch_model.bin:  84%|████████████████   | 1.24G/1.47G [01:28<00:16, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  85%|████████████████▏  | 1.25G/1.47G [01:29<00:15, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████████████▎  | 1.26G/1.47G [01:30<00:15, 14.0MB/s]\u001b[A\n",
      "pytorch_model.bin:  86%|████████████████▍  | 1.27G/1.47G [01:30<00:14, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  87%|████████████████▌  | 1.28G/1.47G [01:31<00:13, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  88%|████████████████▋  | 1.29G/1.47G [01:32<00:12, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████████████▊  | 1.30G/1.47G [01:33<00:11, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  89%|████████████████▉  | 1.31G/1.47G [01:33<00:11, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  90%|█████████████████  | 1.32G/1.47G [01:34<00:10, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████████████▏ | 1.33G/1.47G [01:35<00:09, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  91%|█████████████████▎ | 1.34G/1.47G [01:36<00:08, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  92%|█████████████████▌ | 1.35G/1.47G [01:36<00:08, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  93%|█████████████████▋ | 1.36G/1.47G [01:37<00:07, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████████████▊ | 1.37G/1.47G [01:38<00:06, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  94%|█████████████████▉ | 1.38G/1.47G [01:39<00:05, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  95%|██████████████████ | 1.39G/1.47G [01:39<00:05, 13.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|██████████████████▏| 1.41G/1.47G [01:40<00:04, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  96%|██████████████████▎| 1.42G/1.47G [01:41<00:03, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  97%|██████████████████▍| 1.43G/1.47G [01:42<00:02, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  98%|██████████████████▌| 1.44G/1.47G [01:42<00:02, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|██████████████████▋| 1.45G/1.47G [01:43<00:01, 14.1MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|██████████████████▊| 1.46G/1.47G [01:44<00:00, 14.2MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|███████████████████| 1.47G/1.47G [01:45<00:00, 14.0MB/s]\u001b[A\n",
      "Download complete. Moving file to /root/SD-research/ea_models/llama2/pytorch_model.bin\n",
      "Fetching 4 files: 100%|███████████████████████████| 4/4 [01:55<00:00, 28.91s/it]\n",
      "/root/SD-research/ea_models/llama2\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir '/root/SD-research/base_models/llama2'\n",
    "# !huggingface-cli download yuhuili/EAGLE-llama2-chat-7B --local-dir '/root/SD-research/ea_models/llama2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import yaml\n",
    "import os\n",
    "from eagle.model.ea_model import EaModel\n",
    "from fastchat.model import get_conversation_template\n",
    "import time\n",
    "from statistics import mean\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "/root/SD-research/EAGLE/eagle/model/ea_model.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ea_layer_state_dict = torch.load(load_model_path,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EaModel(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  )\n",
       "  (ea_layer): Model(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "    (act): SiLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EaModel.from_pretrained(\n",
    "    base_model_path='/root/SD-research/base_models/vicuna',\n",
    "    ea_model_path='/root/SD-research/ea_models/vicuna',\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results: List[Dict], \n",
    "                 output_filename: str, \n",
    "                 config_file: str, \n",
    "                 config: Dict,\n",
    "                 output_dir = None,\n",
    "                 avg_acceptance_rate=None,\n",
    "                 avg_token_sec = None,\n",
    "                 total_rejected_token_contexts=None,\n",
    "                 final_latency_results=None):\n",
    "    \"\"\"Save results to CSV and JSON files, including config file reference.\"\"\"\n",
    "    # Extract the base name of the config file (e.g., \"config.yaml\" -> \"config\")\n",
    "    config_name = Path(config_file).stem\n",
    "    timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "    \n",
    "    # File paths with config file name\n",
    "    json_path = output_dir / f\"{output_filename}_{config_name}_{timestamp}.json\"\n",
    "    clean_json_path = output_dir / f\"clean_{output_filename}_{config_name}_{timestamp}.json\"\n",
    "    \n",
    "    # Save results and configuration as JSON\n",
    "    results_with_config = {\n",
    "        'exp_name': output_filename,\n",
    "        'average_token_per_sec': avg_token_sec,\n",
    "        'average_acceptance_rate': avg_acceptance_rate,\n",
    "        'average_latency_by_phase': final_latency_results,\n",
    "        'total_rejected_token_contexts': total_rejected_token_contexts,\n",
    "        \"config\": config,  # Embed the configuration in the JSON\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    clean_results = {\n",
    "        'exp_name': output_filename,\n",
    "        'average_token_per_sec': avg_token_sec,\n",
    "        'average_acceptance_rate': avg_acceptance_rate,\n",
    "        'average_latency_by_phase': final_latency_results,\n",
    "        'total_rejected_token_contexts': total_rejected_token_contexts\n",
    "    }\n",
    "    \n",
    "    with open(clean_json_path, 'w') as f:\n",
    "        json.dump(clean_results, f, indent=2)\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results_with_config, f, indent=2)\n",
    "\n",
    "    # logger.info(f\"Results saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_list(lst, num):\n",
    "    if num not in lst:\n",
    "        return lst\n",
    "    first_index = lst.index(num)\n",
    "    return lst[:first_index + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [12:49<38:28, 769.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.37766400522225 92.35047840227244 3.781207772054285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [27:39<28:00, 840.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.782976542932154 86.19873214397346 3.632136237680685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [44:04<15:06, 906.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.631083731703512 84.84719936319895 3.5910582458725457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [52:42<00:00, 790.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.217228808676397 61.05481053008887 4.069666688159131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "results = []\n",
    "for i in tqdm(range(4)):\n",
    "    prompts = []    \n",
    "    results = []\n",
    "    # load input data\n",
    "    segment_string = f'{500*i}~{500*(i+1)}'\n",
    "    input_filename = f\"../data/ArXiv/4K/ArXiv_4K_{segment_string}.json\"\n",
    "    exp_name = f\"EAGLE_vicuna_7b_v1.3_{segment_string}.json\"\n",
    "    with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "        raw_input = json.load(f)\n",
    "        input_data = [item['final_input'] for item in raw_input]\n",
    "        temperature = 0.0        \n",
    "        tps_eas = []\n",
    "        tps_ars = []\n",
    "        ea_ar_speedups = []\n",
    "        for item in input_data :\n",
    "            conv = get_conversation_template(\"vicuna\")\n",
    "            conv.append_message(conv.roles[0], item)\n",
    "            conv.append_message(conv.roles[1], None)        \n",
    "            prompt = conv.get_prompt()\n",
    "                        \n",
    "            input_ids = model.tokenizer([prompt]).input_ids\n",
    "            input_ids = torch.as_tensor(input_ids).cuda()\n",
    "            input_len = input_ids.shape[1]\n",
    "            naive_text = []\n",
    "            cu_len = input_len\n",
    "            totaltime=0\n",
    "            start_time=time.time()\n",
    "            total_ids=0\n",
    "            \n",
    "            for output_ids in model.ea_generate(input_ids, temperature=temperature, max_steps=512):\n",
    "                totaltime+=(time.time()-start_time)\n",
    "                total_ids+=1\n",
    "                decode_ids = output_ids[0, input_len:].tolist()\n",
    "                decode_ids = truncate_list(decode_ids, model.tokenizer.eos_token_id)\n",
    "                text = model.tokenizer.decode(decode_ids, skip_special_tokens=True, spaces_between_special_tokens=False,\n",
    "                                            clean_up_tokenization_spaces=True, )\n",
    "                # naive_text.append(model.tokenizer.decode(output_ids[0, cu_len], skip_special_tokens=True,\n",
    "                #                                         spaces_between_special_tokens=False,\n",
    "                #                                         clean_up_tokenization_spaces=True, ))\n",
    "\n",
    "                cu_len = output_ids.shape[1]\n",
    "                new_tokens = cu_len-input_len\n",
    "                start_time = time.time()   \n",
    "                \n",
    "            tps_ea = new_tokens/totaltime\n",
    "            tps_eas.append(tps_ea)       \n",
    "                        \n",
    "            for output_ids in model.naive_generate(input_ids, temperature=temperature, max_steps=512):\n",
    "                totaltime += (time.time() - start_time)\n",
    "                total_ids+=1\n",
    "                decode_ids = output_ids[0, input_len:].tolist()\n",
    "                decode_ids = truncate_list(decode_ids, model.tokenizer.eos_token_id)\n",
    "                text = model.tokenizer.decode(decode_ids, skip_special_tokens=True, spaces_between_special_tokens=False,\n",
    "                                            clean_up_tokenization_spaces=True, )\n",
    "                # naive_text.append(model.tokenizer.decode(output_ids[0, cu_len], skip_special_tokens=True,\n",
    "                #                                         spaces_between_special_tokens=False,\n",
    "                #                                         clean_up_tokenization_spaces=True, ))\n",
    "                cu_len = output_ids.shape[1]\n",
    "                new_tokens = cu_len - input_len\n",
    "                tps_ars.append(new_tokens/totaltime)            \n",
    "                start_time = time.time()\n",
    "                \n",
    "            tps_ar = new_tokens/totaltime\n",
    "            tps_ars.append(tps_ar)       \n",
    "            \n",
    "            ea_ar_speedups.append(tps_ea/tps_ar)\n",
    "            # print(ea_ar_speedups)\n",
    "    tps_ar_avg = mean(tps_ars)\n",
    "    tps_ea_avg = mean(tps_eas)\n",
    "    speedup_avg = mean(ea_ar_speedups)\n",
    "    \n",
    "    print(tps_ar_avg, tps_ea_avg, speedup_avg)\n",
    "    \n",
    "    results.append(\n",
    "        {\n",
    "        \"exp_name\": exp_name,\n",
    "        \"avg_speedup_ratio\": round(speedup_avg, 3),\n",
    "        \"avg_token_per_sec_eagle\": round(tps_ea_avg, 3),\n",
    "        \"avg_token_per_sec_ar\": round(tps_ar_avg, 3)\n",
    "        }        \n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "output_file = \"../results/EAGLE/vicuna-7B-v1.3/FINAL_result_summary.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'exp_name': 'EAGLE_vicuna_7b_v1.3_1500~2000.json',\n",
       "  'avg_speedup_ratio': 4.07,\n",
       "  'avg_token_per_sec_eagle': 61.055,\n",
       "  'avg_token_per_sec_ar': 15.217}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "results = []\n",
    "for i in tqdm(range(4)):\n",
    "    prompts = []    \n",
    "    # load input data\n",
    "    segment_string = f'{500*i}~{500*(i+1)}'\n",
    "    input_filename = f\"../data/ArXiv/4K/ArXiv_4K_{segment_string}.json\"\n",
    "    exp_name = f\"EAGLE_vicuna_7b_v1.3_{segment_string}.json\"\n",
    "    with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "        raw_input = json.load(f)\n",
    "        input_data = [item['final_input'] for item in raw_input]\n",
    "        temperature = 0.0        \n",
    "        tps_eas = []\n",
    "        tps_ars = []    \n",
    "        total_tps_ea = 0.0\n",
    "        total_tps_ar = 0.0\n",
    "        ea_ar_speedups = []\n",
    "        for item in input_data :\n",
    "            conv = get_conversation_template(\"vicuna\")\n",
    "            conv.append_message(conv.roles[0], item)\n",
    "            conv.append_message(conv.roles[1], None)        \n",
    "            prompt = conv.get_prompt()\n",
    "                        \n",
    "            input_ids = model.tokenizer([prompt]).input_ids\n",
    "            input_ids = torch.as_tensor(input_ids).cuda()\n",
    "            input_len = input_ids.shape[1]\n",
    "            naive_text = []\n",
    "            cu_len = input_len\n",
    "            totaltime=0\n",
    "            start_time=time.time()\n",
    "            # total_ids=0\n",
    "            \n",
    "            for output_ids in model.ea_generate(input_ids, temperature=temperature, max_steps=512):\n",
    "                totaltime+=(time.time()-start_time)\n",
    "                total_ids+=1\n",
    "                decode_ids = output_ids[0, input_len:].tolist()\n",
    "                decode_ids = truncate_list(decode_ids, model.tokenizer.eos_token_id)\n",
    "                text = model.tokenizer.decode(decode_ids, skip_special_tokens=True, spaces_between_special_tokens=False,\n",
    "                                            clean_up_tokenization_spaces=True, )\n",
    "                # naive_text.append(model.tokenizer.decode(output_ids[0, cu_len], skip_special_tokens=True,\n",
    "                #                                         spaces_between_special_tokens=False,\n",
    "                #                                         clean_up_tokenization_spaces=True, ))\n",
    "\n",
    "                cu_len = output_ids.shape[1]\n",
    "                new_tokens = cu_len-input_len\n",
    "                start_time = time.time()   \n",
    "                \n",
    "            tps_ea = new_tokens/totaltime\n",
    "            total_tps_ea += tps_ea\n",
    "                        \n",
    "            for output_ids in model.naive_generate(input_ids, temperature=temperature, max_steps=512):\n",
    "                totaltime += (time.time() - start_time)\n",
    "                total_ids+=1\n",
    "                decode_ids = output_ids[0, input_len:].tolist()\n",
    "                decode_ids = truncate_list(decode_ids, model.tokenizer.eos_token_id)\n",
    "                text = model.tokenizer.decode(decode_ids, skip_special_tokens=True, spaces_between_special_tokens=False,\n",
    "                                            clean_up_tokenization_spaces=True, )\n",
    "                # naive_text.append(model.tokenizer.decode(output_ids[0, cu_len], skip_special_tokens=True,\n",
    "                #                                         spaces_between_special_tokens=False,\n",
    "                #                                         clean_up_tokenization_spaces=True, ))\n",
    "                cu_len = output_ids.shape[1]\n",
    "                new_tokens = cu_len - input_len\n",
    "                tps_ars.append(new_tokens/totaltime)            \n",
    "                start_time = time.time()\n",
    "                \n",
    "            tps_ar = new_tokens/totaltime\n",
    "            total_tps_ar += tps_ar\n",
    "            # print(ea_ar_speedups)\n",
    "    # tps_ar_avg = mean(tps_ars)\n",
    "    # tps_ea_avg = mean(tps_eas)\n",
    "    # speedup_avg = mean(ea_ar_speedups)\n",
    "    \n",
    "    tps_ea_avg = total_tps_ea / len(input_data) # 80\n",
    "    tps_ar_avg = total_tps_ar / len(input_data) # 80\n",
    "    speedup_avg = tps_ea_avg / tps_ar_avg\n",
    "    \n",
    "    print(tps_ar_avg, tps_ea_avg, speedup_avg)\n",
    "    \n",
    "    results.append(\n",
    "        {\n",
    "        \"exp_name\": exp_name,\n",
    "        \"avg_speedup_ratio\": round(speedup_avg, 3),\n",
    "        \"avg_token_per_sec_eagle\": round(tps_ea_avg, 3),\n",
    "        \"avg_token_per_sec_ar\": round(tps_ar_avg, 3)\n",
    "        }        \n",
    "    )   \n",
    "    \n",
    "output_file = \"../results/EAGLE/vicuna-7B-v1.3/FINAL_result_summary_novar.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12739"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will receive the full text of a scientific paper. Your task is to write a formal abstract (under 300 words) that includes:\n",
      "\n",
      "- The primary objective or research question.\n",
      "- A brief description of the methodology.\n",
      "- The main findings or results.\n",
      "- The significance or implications of the findings.\n",
      "\n",
      "Use clear, professional, and concise language suitable for publication.\n",
      "\n",
      "### Paper content\n",
      "\n",
      "there are several reasons to believe that a population of intergalactic globular clusters ( igcs ) should exist outside of galaxies :    \\(1 ) the jeans mass at recombination was @xmath0 solar masses , and hence globular cluster sized objects could have formed wherever the local density of matter was high enough . \\(2 ) many galaxies may have met their demise over a hubble time as a result of collisions and tidal disruption . globular clusters are likely to survive the disruption of their parent galaxy , resulting in the gradual accumulation of a population of igcs . intergalactic stars , planetary nebulae , supernovae and hii regions have already been found ; it would be surprising if there were no igcs . \\(3 ) the existence of igcs might explain high specific frequencies , bimodal globular cluster metallicity distributions and other current puzzles in the study of globular cluster systems . jordn et al . ( 2003 ) reported a tentative detection of igcs in the center of the rich galaxy cluster a1185 ( @xmath1 ) based on @xmath2-band images obtained with wfpc2 on the hubble space telescope . we ( ct , jordn , marzke , west ) recently obtained very deep , multicolored ( @xmath3 and @xmath2 ) images of the same a1185 field using hst with the new acs . the goals of these new observations are to 1 ) detect the peak of the assumed universal gaussian - like globular cluster luminosity function ( which should occur at @xmath4 at a1185 s distance ) and thereby confirm that these candidate igcs are bona fide globular clusters and 2 ) use color information to infer their metallicities . preliminary analysis indicates that we are reaching sufficiently faint magnitudes to reliably detect the luminosity function turnover . the number and colors ( metallicities ) of igcs will provide constraints on the number and types of galaxies that have been destroyed or stripped over a hubble time .    using the keck telescope , we ( ferguson , gregg , tanvir , von hippel , west ) recently measured the redshift of a candidate igc in the nearby virgo galaxy cluster that was found serendipitously on an hst image obtained for another project . preliminary data reductions show that this object , which is slightly resolved in the hst image and appears to be a distant globular cluster , has a recessional velocity of @xmath5 km / s , and hence is most likely in the virgo cluster . its apparent magnitude , @xmath6 , is consistent with it being a bright globular cluster . using telescopes on mauna kea we have since obtained optical and nir colors of this object , as well as a medium - resolution spectrum that should yield its velocity dispersion . these data are presently being analyzed .\n",
      "\n",
      "### Output\n",
      "\n",
      "Abstract: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: You will receive the full text of a scientific paper. Your task is to write a formal abstract (under 300 words) that includes:\\n\\n- The primary objective or research question.\\n- A brief description of the methodology.\\n- The main findings or results.\\n- The significance or implications of the findings.\\n\\nUse clear, professional, and concise language suitable for publication.\\n\\n### Paper content\\n\\nthere are several reasons to believe that a population of intergalactic globular clusters ( igcs ) should exist outside of galaxies :    \\\\(1 ) the jeans mass at recombination was @xmath0 solar masses , and hence globular cluster sized objects could have formed wherever the local density of matter was high enough . \\\\(2 ) many galaxies may have met their demise over a hubble time as a result of collisions and tidal disruption . globular clusters are likely to survive the disruption of their parent galaxy , resulting in the gradual accumulation of a population of igcs . intergalactic stars , planetary nebulae , supernovae and hii regions have already been found ; it would be surprising if there were no igcs . \\\\(3 ) the existence of igcs might explain high specific frequencies , bimodal globular cluster metallicity distributions and other current puzzles in the study of globular cluster systems . jordn et al . ( 2003 ) reported a tentative detection of igcs in the center of the rich galaxy cluster a1185 ( @xmath1 ) based on @xmath2-band images obtained with wfpc2 on the hubble space telescope . we ( ct , jordn , marzke , west ) recently obtained very deep , multicolored ( @xmath3 and @xmath2 ) images of the same a1185 field using hst with the new acs . the goals of these new observations are to 1 ) detect the peak of the assumed universal gaussian - like globular cluster luminosity function ( which should occur at @xmath4 at a1185 s distance ) and thereby confirm that these candidate igcs are bona fide globular clusters and 2 ) use color information to infer their metallicities . preliminary analysis indicates that we are reaching sufficiently faint magnitudes to reliably detect the luminosity function turnover . the number and colors ( metallicities ) of igcs will provide constraints on the number and types of galaxies that have been destroyed or stripped over a hubble time .    using the keck telescope , we ( ferguson , gregg , tanvir , von hippel , west ) recently measured the redshift of a candidate igc in the nearby virgo galaxy cluster that was found serendipitously on an hst image obtained for another project . preliminary data reductions show that this object , which is slightly resolved in the hst image and appears to be a distant globular cluster , has a recessional velocity of @xmath5 km / s , and hence is most likely in the virgo cluster . its apparent magnitude , @xmath6 , is consistent with it being a bright globular cluster . using telescopes on mauna kea we have since obtained optical and nir colors of this object , as well as a medium - resolution spectrum that should yield its velocity dispersion . these data are presently being analyzed .\\n\\n### Output\\n\\nAbstract:  ASSISTANT:\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "segment_string = f'{500*i}~{500*(i+1)}'\n",
    "input_filename = f\"../data/ArXiv/4K/ArXiv_4K_{segment_string}.json\"\n",
    "with open(input_filename, 'r', encoding='utf-8') as f:\n",
    "    raw_input = json.load(f)\n",
    "    input_data = [item['final_input'] for item in raw_input]        \n",
    "    print(input_data[0])\n",
    "\n",
    "conv = get_conversation_template(\"vicuna\")\n",
    "conv.append_message(conv.roles[0], input_data[0])\n",
    "conv.append_message(conv.roles[1],None)\n",
    "prompt = conv.get_prompt()\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=model.tokenizer([prompt]).input_ids\n",
    "input_ids = torch.as_tensor(input_ids).cuda()\n",
    "output_ids=model.eagenerate(input_ids,temperature=0.5,max_new_tokens=512)\n",
    "output=model.tokenizer.decode(output_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   319, 13563,  ...,  6757, 29889,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: You will receive the full text of a scientific paper. Your task is to write a formal abstract (under 300 words) that includes:\\n\\n- The primary objective or research question.\\n- A brief description of the methodology.\\n- The main findings or results.\\n- The significance or implications of the findings.\\n\\nUse clear, professional, and concise language suitable for publication.\\n\\n### Paper content\\n\\nthere are several reasons to believe that a population of intergalactic globular clusters ( igcs ) should exist outside of galaxies :    \\\\(1 ) the jeans mass at recombination was @xmath0 solar masses , and hence globular cluster sized objects could have formed wherever the local density of matter was high enough . \\\\(2 ) many galaxies may have met their demise over a hubble time as a result of collisions and tidal disruption . globular clusters are likely to survive the disruption of their parent galaxy , resulting in the gradual accumulation of a population of igcs . intergalactic stars , planetary nebulae , supernovae and hii regions have already been found ; it would be surprising if there were no igcs . \\\\(3 ) the existence of igcs might explain high specific frequencies , bimodal globular cluster metallicity distributions and other current puzzles in the study of globular cluster systems . jordn et al . ( 2003 ) reported a tentative detection of igcs in the center of the rich galaxy cluster a1185 ( @xmath1 ) based on @xmath2-band images obtained with wfpc2 on the hubble space telescope . we ( ct , jordn , marzke , west ) recently obtained very deep , multicolored ( @xmath3 and @xmath2 ) images of the same a1185 field using hst with the new acs . the goals of these new observations are to 1 ) detect the peak of the assumed universal gaussian - like globular cluster luminosity function ( which should occur at @xmath4 at a1185 s distance ) and thereby confirm that these candidate igcs are bona fide globular clusters and 2 ) use color information to infer their metallicities . preliminary analysis indicates that we are reaching sufficiently faint magnitudes to reliably detect the luminosity function turnover . the number and colors ( metallicities ) of igcs will provide constraints on the number and types of galaxies that have been destroyed or stripped over a hubble time .    using the keck telescope , we ( ferguson , gregg , tanvir , von hippel , west ) recently measured the redshift of a candidate igc in the nearby virgo galaxy cluster that was found serendipitously on an hst image obtained for another project . preliminary data reductions show that this object , which is slightly resolved in the hst image and appears to be a distant globular cluster , has a recessional velocity of @xmath5 km / s , and hence is most likely in the virgo cluster . its apparent magnitude , @xmath6 , is consistent with it being a bright globular cluster . using telescopes on mauna kea we have since obtained optical and nir colors of this object , as well as a medium - resolution spectrum that should yield its velocity dispersion . these data are presently being analyzed .\\n\\n### Output\\n\\nAbstract:  ASSISTANT:\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The primary objective of this study was to investigate the existence of intergalactic globular clusters (IGCs) outside of galaxies. The research question was whether IGCs exist and could be detected in the field of the rich galaxy cluster A1185. The methodology involved acquiring deep, multicolored images of the A1185 field using the Hubble Space Telescope (HST) and the new Advanced Camera for Surveys (ACS). The authors aimed to detect the peak of the assumed universal Gaussian-like globular cluster luminosity function and use color information to infer the metallicities of the IGCs. The main findings were that the authors were able to reach sufficiently faint magnitudes to reliably detect the luminosity function turnover. Preliminary analysis of the data revealed that the number and colors of the IGCs will provide constraints on the number and types of galaxies that have been destroyed or stripped over a Hubble time. The significance of the findings is that the existence of IGCs might explain high specific frequencies, bimodal globular cluster metallicity distributions, and other current puzzles in the study of globular cluster systems.</s>'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('ASSISTANT: ',output)[-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
